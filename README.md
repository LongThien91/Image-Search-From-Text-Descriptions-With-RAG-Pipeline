# Image-Search-From-Text-Descriptions-With-RAG-Pipeline

## üìå  Project Overview

Image Search From Text Descriptions With RAG Pipeline is an intelligent image retrieval system that allows users to search for images using natural language queries. The system combines the power of Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) to deliver accurate and context-aware image search results.

## üîç How It Works
1. Image Upload & Description Generation
When an image is uploaded, a Large Language Model (LLM) is used to automatically generate a detailed textual description of the image content. Alternatively, users can provide their own custom descriptions for improved accuracy.

2. Vectorization & Storage
The description (generated or user-provided) is embedded into a dense vector using a sentence embedding model and stored in Qdrant, a high-performance vector database. The image itself is uploaded to Cloudinary, and its ID is linked to the corresponding vector in Qdrant.

3. Text-Based Image Retrieval
When a user submits their query (e.g., "a black t-shirt with a big heart and the YOLO logo on it"), the system embeds the query and performs a semantic similarity search in Qdrant to retrieve the top-k most relevant image descriptions.

4. Post-Filtering with LLM for Best Match
After retrieving the top-matching descriptions, the system uses the LLM again to compare each description with the user query  and remove any results that are not really related. Finally, all the best-matching images are selected and all Cloudinary URL are returned to the user.

## üñºÔ∏è System Architecture

The diagram below illustrates the complete pipeline - from uploading an image to delivering the best matching result based on a natural language query:

<p align="center">
  <img src="Assets\Screenshot 2025-05-22 141707.jpg" alt="System Architecture">
</p>

Explanation:
Image + Description: The user uploads images, its will be saved on Cloudinary . Descriptions is either generated by the LLM or provided by the user.

Embedding: The description is embedded into a vector via an embedding model and stored in the Qdrant vector database.

Query: A user submits a text query, which is also embedded and used to search for the most semantically similar image descriptions.

Reranking: The top relevant results are passed through an LLM reranking step to ensure the final match is meaningful.

Image Retrieval: The best-matching image is retrieved from Cloudinary and returned.

## ‚ö†Ô∏è Limitations & Future Improvements
### üîª Current Limitations

While the system demonstrates effective image search for fashion related images, there are still several limitations:

Prompt Specificity: The current prompt used to generate image descriptions is mainly optimized for clothing and apparel. As a result, descriptions for other object types may lack detail or relevance.

LLM Output Quality: Some image descriptions generated by the LLM may be too generic or lacking in key details, which can reduce the accuracy of the semantic vector search in Qdrant.

Limited Generalization: The system may not perform well on images containing objects, scenes, or items outside the fashion domain unless the prompt and description generation process is carefully adjusted.

### üîß Future Improvements

To address the above limitations and improve performance, the following directions are considered:

Prompt Engineering: Refine and adapt prompts dynamically based on image content or domain to produce more detailed and accurate descriptions.

Better Embedding Models: Experiment with more advanced or domain-specific embedding models to improve semantic understanding.

Alternative LLMs: Use or fine-tune local LLMs (e.g., Mistral, LLaMA, Vintern ...) to reduce dependency on external APIs and enhance control over output quality.